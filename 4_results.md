# AI Model Comparison Results

The following are the results I observed from tests with my prompt:

- Manus AI: Extremely good use of the rules and logic with the use of a perfect formatting and feedback.

  Access at: [Manus Chat]()
  Link: 

- ChatGPT (GPT-5): Extremely good use of the rules and logic with the use of a perfect formatting and feedback.

  Access at: [ChatGPT Chat]()
  Link: 

- Claude Haiku 4.5: Extremely good use of the rules and logic with the use of a good formatting and feedback. Misses the desired bold on section titles.

  Access at: [Claude Haiku Chat]()
  Link: 

- Claude Sonnet 4.5: Extremely good use of the rules and logic with the use of a good formatting and feedback. Misses the desired bold on section titles.

  Access at: [Claude Sonnet Chat]()
  Link: 

- DeepSeek: Good use of the rules and logic with the use of a good formatting and feedback. Misses the desired bold on section titles.

  Access at: [DeepSeek Chat]()
  Link: 

- DeepSeek (DeepThink mode ON): Extremely good use of the rules and logic with the use of a good formatting and feedback. Misses the desired bold on section titles and is extremely slow.

  Access at: [DeepSeek and Think Chat]()
  Link: 

- Gemini 2.5 Flash: Extremely good use of the rules, correct feedback and good logic. Poor handling of the separation between section titles.

  Access at: [Gemini Flash Chat]()
  Link: 

- Gemini 2.5 Pro: Good use of the rules, correct feedback and good logic. The corrected text is being returned in full bold and with bad handling of separation between paragraphs.

  Access at: [Gemini Pro Chat]()
  Link: 

- Meta AI: Missing corrections and bad feedback.

  Access at: [Meta AI Chat]()
  Link: 
