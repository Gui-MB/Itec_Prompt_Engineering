# AI Model Comparison Results

The following are the results I observed from tests with my prompt:

- Manus AI: Extremely good use of the rules and logic with the use of a perfect formatting and feedback.

  Access at: [Manus Chat]() or ``
  
- ChatGPT (GPT-5): Extremely good use of the rules and logic with the use of a perfect formatting and feedback.

  Access at: [ChatGPT Chat]() or ``
  
- Claude Haiku 4.5: Extremely good use of the rules and logic with the use of a good formatting and feedback. Misses the desired bold on section titles.

  Access at: [Claude Haiku Chat]() or ``

- Claude Sonnet 4.5: Extremely good use of the rules and logic with the use of a good formatting and feedback. Misses the desired bold on section titles.

  Access at: [Claude Sonnet Chat]() or ``
  
- DeepSeek: Good use of the rules and logic with the use of a good formatting and feedback. Misses the desired bold on section titles.

  Access at: [DeepSeek Chat]() or ``
  
- DeepSeek (DeepThink mode ON): Extremely good use of the rules and logic with the use of a good formatting and feedback. Misses the desired bold on section titles and is extremely slow.

  Access at: [DeepSeek and Think Chat]() or ``
  
- Gemini 2.5 Flash: Extremely good use of the rules, correct feedback and good logic. Poor handling of the separation between section titles.

  Access at: [Gemini Flash Chat]() or ``
  
- Gemini 2.5 Pro: Good use of the rules, correct feedback and good logic. The corrected text is being returned in full bold and with bad handling of separation between paragraphs.

  Access at: [Gemini Pro Chat]() or ``
  
- Meta AI: Missing corrections and bad feedback.

  Access at: [Meta AI Chat]() or ``
